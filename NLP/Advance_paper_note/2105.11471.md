# 文献阅读总结: 
## A Constructive Framework for Self-Supervised Sentence Representation Transfer
#### 文献内容: 
#### 文献地址: https://arxiv.org/pdf/2105.11741.pdf

#### 1. 研究背景
   BERT 模型在自然语言处理领域的多个任务中取得了强大的性能. 但是在语义文本相似度理解上, BERT模型的文本表示性能相对较差.几乎所有文本都被赋予了 0.6 - 1.0 的相互之间的相似性. 这意味着 BERT 模型会将所有句子统一投射到一个相对较小的区域空间内.
#### 2. 研究方法
###### 2.1 总体框架
 * 在 token embedding layer 为输入的数据生成不同视野的数据增强函数.
 * 构建了一个共享的 BERT encoder. 用生成的最后一个 avg 池化层输出作为 token 的向量表示.
 * 构建了 BERT 的顶部对比器. 提高了句子向量表示的版本一致性, 又使每个句子之间保持一定的距离.
###### 2.2 数据增强方法
 * FGV. 用于实现最大限度的扰动, 依赖于 loss 的监督.
 * 打乱 token 的 id 顺序.
 * Dropout. 利用正则化规避过拟合.
 * Cutoff. 用于 token 和 scale .
###### 2.3 加入监督信息
 * 利用 Ljoint = Lce + Lcon 对模型进行联合训练.
#### 3. 参数设置
#### 4. 性能及结果
