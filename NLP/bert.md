# 学习笔记: NLP 中的 BERT 模型

### 1. BERT 的核心机制.
    PASS
#### 1.1 Attention 机制.
    PASS
#### 1.2 Transformer 机制.
    PASS

### 2. BERT 的训练方式
#### 2.1 训练流程
    BERT 模型的训练分为两个部分: 预训练 和 语言模型训练.  
* 预训练  
     预训练是针对较大的语料库完成的基础语料准备的训练方法. 只需要利用庞大的语料数据训练一个  
    泛化能力较强的模型, 在后续使用的时候对参数进行微调, 并利用使用时的数据进行数据增强即可.
* 语言模型训练  
     语言模型训练是在已经有预训练模型的基础上, 针对实际使用的语料库进行数据增强并修改输出层  
    参数时得到的可用于后续下游任务的语言模型.
#### 2.2 语料预训练: MLM
######预训练的任务 1: Maksed Language Model  
机器会随机将一个文本 15% 的词汇利用 '[MASK]' 符号作为遮蔽使机器进行上下文预测.   
    即: 利用非监督学习的方法让模型预测上下文的词
  * for example  
    my dog is hairy but my cat is not. -> my [MASK] is hairy but ...
  * 遮蔽规则如下:
     * 一个文本的 15% 的词汇作为目标被预测
     * 这 15% 的词汇中, 80% 用'[MASK]' 替换进行上下文预测操作.
     * 剩余的有 10% 的词汇会被随机词汇替代进行上下文预测学习.
     * 剩余 10% 的词汇不变, 继续利用原有词汇进行预测学习.
    
###### 使用随机词的好处:  
     使用随机词语可以保持 transformer 机制对分布式词汇的表征. 用人话说就是, 利用随机词汇可以不让  
    被遮蔽的词汇被机器 '记住' 而是被 '学到'. 而对于其可能产生的误差, 由于生成随机词汇的概率为 1.5%,  
    在数据集中的占比并不大, 认为利大于弊.

###### 重复操作的意义:
     在 MLM 中, BERT 会让同一个文本重复多次进行随机遮蔽任务. 这一行为的好处是能让 transformer 机制  
    获得更优的全局视野. 即可以让机器对上下文的分布更为了解, 后续的语言模型表达也更为灵活准确. 

#### 2.3 语料预训练: NSP
###### 预训练的任务2: Next Sentence Prediction
机器会对文本选择句子对. 用于机器预测上下句是否存在对应关系.

  * 分句规则:   
    50% 的句子对有上下文关系, 而另外的 50% 没有上下文关系.
  * 分句来源:  
    利用 MLM 任务中基于重复多次得到的, 拥有较好全局视野的句子进行预测.

###### 为什么要这样做:
     为了让模型得到的结果能够适应于各种各样的下游任务. 下游任务中有比较多的部分都会涉及到前后文关系  
    的梳理. 提前让模型学习相关性能更好地解决这个问题.

#### 2.4 语言模型训练
    利用预训练好的语言模型, 修改参数并放入新的文本即可得到由一个或多个特征向量构成的语言模型.

### 3. BERT 的贡献
  * 使用双向LM做模型预训练.
  * 为预训练引入了新目标NSP, 它可以学习句子与句子间的关系.
  * 为下游任务引入了很通用的求解框架, 不再为任务做模型定制.
  * 刷新了多项NLP任务的记录.

### 4. BERT 的优缺点
##### 优点
  * BERT 的泛化性能极强. 输入输出层的设计也较为简单, 适合多任务.  
  * BERT 在预训练阶段由于利用了 Attention 机制, 语言模型的实际考量效果是双向的.
  * BERT 的微调简便, 成本小.
##### 缺点
  * BERT 的算力开销过大.
  * BERT 在预训练过程中由于利用了 '[MASK]' 使得最终收敛较慢.

###### 参考资料:
    https://blog.csdn.net/sunhua93/article/details/102764783  
    https://blog.csdn.net/yangfengling1023/article/details/84025313?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.control&spm=1001.2101.3001.4242
    